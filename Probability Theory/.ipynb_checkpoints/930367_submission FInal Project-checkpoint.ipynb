{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "## Final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final project you will develop your own classification algorithm based on probabilistic models.\n",
    "\n",
    "In the `data` folder you can find a collection of texts in English, Italian, Spanish, German, French, Polish and Portuguese languages obtained from random Wikipedia articles, see e.g. `Spanish.txt`. (See corresponding `.source.txt` files and links therein for lists of authors.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English.sources.txt  German.txt\t\t  Portuguese.sources.txt\r\n",
      "English.txt\t     Italian.sources.txt  Portuguese.txt\r\n",
      "French.sources.txt   Italian.txt\t  Spanish.sources.txt\r\n",
      "French.txt\t     Polish.sources.txt   Spanish.txt\r\n",
      "German.sources.txt   Polish.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomas zapata sierra medellin colombia  de mayo de  es productor de cine y de teatrocita requerida\n",
      "sad eyed lady of the lowlands en espanol senorita de ojos tristes de las tierras bajas es una cancion compuesta por el cantante estadounidense bob dylan fue incluida en el album blonde on blonde editado el  de mayo de \n",
      "la revista mojo la coloco en el puesto  de su lista de las  mejores canciones de bob dylan\n",
      "calyptocephalella canqueli es una especie extinta de anfibio anuro perteneciente al genero c\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Spanish.txt\") as f:\n",
    "    print(f.read(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riedhofe ist der name von ortsteilen in deutschland\n",
      "\n",
      "in badenwurttemberg\n",
      "riedhofe langenau ortsteil der stadt langenau im albdonaukreis\n",
      "riedhofe frickingen ortsteil der gemeinde frickingen im bodenseekreis\n",
      "riedhofe riegel am kaiserstuhl ortsteil der gemeinde riegel am kaiserstuhl im landkreis emmendingen\n",
      "riedhofe kongen ortsteil der gemeinde kongen im landkreis esslingen\n",
      "riedhofe leingarten ortsteil der gemeinde leingarten im landkreis heilbronn\n",
      "riedhofe bad wurzach ortsteil der stadt bad wurzac\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/German.txt\") as f:\n",
    "    print(f.read(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our training data. These texts are preprocessed: only standard Latin characters kept, diacritics removed, punctuations removed, all letters converted to lowercase. We will use similar preprocessing for new texts that we have to classify. Here are some useful functions to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "### FROM: https://stackoverflow.com/a/518232/3025981\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "### END FROM\n",
    "\n",
    "def clean_text(s):\n",
    "    return re.sub(\"[^a-z \\n]\", \"\", strip_accents(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning: obtaining character frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to find character relative frequencies in texts of each language. We will consider them as probability for character to appear in our multinomial model. Write function `get_freqs(text, relative)` that takes string `text` as input and returns dictionary which keys are all distinct characters occurred in `text` and values are frequencies (relative if `relative` is `True` and absolute otherwise). A similar function was discussed in Python videos previously. Note that Python treat `str` objects as a sequence of characters (e.g. if you try to iterate it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a31da5fc73be6eee36e6a8d49d08885",
     "grade": false,
     "grade_id": "cell-bcf26fb5354210c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_freqs(text, relative=False):\n",
    "    counter = Counter(text)\n",
    "    \n",
    "    if relative:\n",
    "        total_count = sum(counter.values())\n",
    "        relative_counter = {}\n",
    "        for key in counter:\n",
    "            relative_counter[key] = counter[key] / total_count\n",
    "        return relative_counter\n",
    "    else:\n",
    "        return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea90921753fb298894ed69e10cdaea3a",
     "grade": true,
     "grade_id": "cell-122fc498050c5088",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert get_freqs('Hello, World!') == {'H': 1, 'e': 1, 'l': 3, 'o': 2, ',': 1,\n",
    "                                      ' ': 1, 'W': 1, 'r': 1, 'd': 1, '!': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use function `get_freqs` to create a dictionary `lang_to_prob` which keys are names of languages (i.e. `'English'`, `'Italian'`, `'Spanish'`, `'German'`, `'French'`, `'Polish'`, `'Portuguese'`) and values are dictionaries of relative frequencies, obtained by processing of corresponding `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7cf421e2337fd84ea7d773c2badd767",
     "grade": false,
     "grade_id": "cell-1beba6e8ac03b0b0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "lang_to_probs = {}\n",
    "\n",
    "for lang in ('English', 'French', 'German', 'Italian', 'Polish', 'Portuguese', 'Spanish'):\n",
    "    with open(\"data/{}.txt\".format(lang)) as f:\n",
    "        lang_to_probs[lang] = get_freqs(f.read(), relative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77eb7631d066c05a174abd522974d57e",
     "grade": true,
     "grade_id": "cell-56feb4b2ab62c15c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(lang_to_probs['Polish']['a'] - 0.08504245058355897) < 0.00001\n",
    "assert abs(lang_to_probs['English']['x'] - 0.001387857977519179) < 1e-5\n",
    "assert len(set(lang_to_probs['Portuguese'])) == 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihoods\n",
    "Now let us start implementing the actual classifier. We begin with a multinomial likelihood function. Implement function `multinomial_likelihood(probs, freqs)` that takes two arguments: `probs` are dictionary of probabilities of each character (in some language) and `freqs` is dictionary of absolute frequencies of each character (in some text we want to classify). This function has to return probability to obtain these absolute frequencies from multinomial distribution with given probabilities $P((X_1 = f_1) \\cap (X_2 = f_2) \\cap \\ldots \\cap (X_k = f_k))$ provided that $(X_1, \\ldots, X_k)$ is a system of multinomially distributed values with probabilities $(p_1, \\ldots, p_k)$. (You need the `factorial` function that can be imported from `math`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d1cc2f77a90c6103bc87d746c969174",
     "grade": false,
     "grade_id": "cell-9a0431bd007d04cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# your code here\n",
    "def multinomial_likelihood(probs, freqs):\n",
    "    n = sum(freqs.values())\n",
    "    p1 = math.factorial(n)\n",
    "    p2 = 1\n",
    "    p3 = 1\n",
    "    \n",
    "    for k in probs.keys() & freqs.keys():\n",
    "        p2 *= math.factorial(freqs[k])\n",
    "        p3 *= probs[k] ** freqs[k]\n",
    "\n",
    "    return p1 * p3 / p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find likelihood of data with frequencies `{'a': 2, 'b': 1, 'c': 2}` and probabilities `{'a': 0.2, 'b': 0.5, 'c': 0.3}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05400000000000001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3}, freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "878cba6cdee3f20b13be7533627e0553",
     "grade": true,
     "grade_id": "cell-0a8a6d99346b6bf8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.054) < 0.000001\n",
    "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.1, 'c': 0.3, 'd': 0.4},\n",
    "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.0108) < 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coefficient with factorials depends only on `freqs` (i.e. text that we analyse) and does not depend on `probs`. It means that for all possible languages this coefficient will be the same. As we are going to consider fixed text and compare likelihoods for different languages, we see that we do not need this coefficient in most cases. Let us implement the function `multinomial_likelihood_without_coeff` that returns the same probability as `multinomial_likelihood` but without the coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88623d0902e7a18eac1bd7885c210973",
     "grade": false,
     "grade_id": "cell-99bfc1e01b6b804e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def multinomial_likelihood_without_coeff(probs, freqs):\n",
    "    p = 1\n",
    "    \n",
    "    for k in probs.keys() & freqs.keys():\n",
    "        p *= probs[k] ** freqs[k]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding probability becomes smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0018000000000000004"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                                     freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a71414f54c182fd492b07fb428fc71c6",
     "grade": true,
     "grade_id": "cell-3de0433027c5af68",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert multinomial_likelihood_without_coeff(\n",
    "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
    "    freqs={'a': 2, 'b': 1, 'c': 2}) == 0.00324\n",
    "assert abs(multinomial_likelihood_without_coeff(\n",
    "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
    "    freqs={'a': 2, 'b': 1, 'c': 5}) - 8.747999999999e-05) < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, likelihoods become extremely small very quickly when we increase absolute frequencies in data. It is not surprising: the probability to get the text that coincides with our actual text from a random experiment we discussed is extremely small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00018000000000000004"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 3, 'b': 2, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.866455078125001e-10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 3, 'b': 20, 'c': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limited precision of computer arithmetic, for frequencies large enough we will get exactly zero likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                                     freqs={'a': 543, 'b': 512, 'c': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we usually cannot use likelihoods like this directly. Common way to deal with such a tiny numbers is to use _logarithms_ instead of likelihood themself. Indeed, logarithm is a monotonically increasing function. If we compare logarithms, it is equivalent to compare their arguments.\n",
    "\n",
    "### Log likelihood\n",
    "Implement function `log_likelihood_without_coeff(probs, freqs)` that calculates logarithm of likelihood (without factorial coefficient). Note that you cannot simply put `multinomial_likelihood_without_coeff` into `log`: if the likelihood would be extremely small (equal to zero from the computer's point of view), `log` of it will be negative infinity. Thus we have to algebraically transform the expression for log likelihood first. Let us use properties of logarithm to do it: $\\log (ab) = \\log a + \\log b$, $\\log (a^b)=b\\log a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5519c92583bc940cad0f293a20a80397",
     "grade": false,
     "grade_id": "cell-c6283b47bbcf5b95",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def log_likelihood_without_coeff(probs, freqs):\n",
    "    p = 0\n",
    "    for k in probs.keys() & freqs.keys():\n",
    "            p += freqs[k] * math.log(probs[k])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihoods are probabilities, so they are less than 1 and their logarithms are negative. The larger absolute value of log-likelihood, the less is likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.319968614080018"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deal with the inputs that lead to exact zero value previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1231.2240885070603"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6de307b4a1af6955524f2381c00ec68",
     "grade": true,
     "grade_id": "cell-38d4478fc7c1656b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                              freqs={'a': 2, 'b': 1, 'c': 2}) + 6.319968614080018) < 0.00001\n",
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512, 'c': 2}) + 1231.2240885070605) < 0.0001\n",
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512}) + 1228.8161428984085) < 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical question: cross-entropy and Jensen's inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that you obtained is also known as *cross entropy* and is extremely popular in machine learning, namely, in classification problems. It allows you to measure how good probability distribution $(p_1, \\ldots, p_k)$ fits to actual absolute frequencies obtained from the data $(f_1, \\ldots, f_k)$.\n",
    "\n",
    "Assume that frequencies $(f_1, \\ldots, f_k)$ are fixed. What is the best distribution $(p_1, \\ldots, p_k)$ from likelihood's perspective?\n",
    "\n",
    "Intuitively, it seems that we have to put relative frequencies\n",
    "$$r_i = \\frac{f_i}{\\sum_{j=1}^k f_j}$$ \n",
    "as $p_i$ to get best fit. In fact, it is true. To prove it, let us use Jensen's inequality for logarithms. It is stated as follows:\n",
    "\n",
    "For any values $\\alpha_1, \\ldots, \\alpha_k$, such that $\\sum_{j=1}^k \\alpha_j = 1$ and $\\alpha_j \\ge 0$ for all $j=1, \\ldots, k$, and any positive values $x_1, \\ldots, x_k$, the following inequality holds:\n",
    "\n",
    "$$\\log \\sum_{j=1}^k \\alpha_j x_j \\ge \\sum_{j=1}^k \\alpha_j\\log(x_j).$$\n",
    "\n",
    "Use this inequality to prove that\n",
    "$$\\sum_{j=1}^k r_j \\log p_j - \\sum_{j=1}^k r_j \\log r_j \\le 0,$$\n",
    "then prove that to obtain maximum log-likelihood (and therefore maximum likelihood) for fixed $(f_1, \\ldots, f_k)$ we have to put $p_i=r_i$, $i=1,\\ldots, k$.\n",
    "\n",
    "**Hint:** use properties of logarithm to transform the left-hand part of the last inequality to the right-hand part of the previous inequality.\n",
    "\n",
    "(Submit your proof as a staff graded assignment.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use likelihood to choose the best language for some text we want to classify. Write function `mle_best(text, lang_to_probs)` that takes some `text` and dictionary `lang_to_probs` that we created previously and returns the name of the language such that the likelihood of our data for this language is maximal. Note that you have to preprocess  `text` using the function `clean_text` before finding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "489b2d5186054873ecb269d587c09e37",
     "grade": false,
     "grade_id": "cell-761e62cc0f17dc2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def mle_best(text, lang_to_probs, return_all=False):\n",
    "    freqs = get_freqs(clean_text(text), True)\n",
    "    \n",
    "    p = {}\n",
    "    \n",
    "    for lang, probs in lang_to_probs.items():\n",
    "        p[lang] = log_likelihood_without_coeff(probs=probs, freqs=freqs)\n",
    "    if return_all: \n",
    "        return p\n",
    "    \n",
    "    return max(p, key=lambda key: p[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://en.wikipedia.org/wiki/1134_Kepler\n",
    "text = \"\"\"1134 Kepler, provisional designation 1929 SA, is a stony asteroid \n",
    "and eccentric Mars-crosser from the asteroid belt, approximately \n",
    "4 kilometers in diameter\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polish'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://pl.wikipedia.org/wiki/(1134)_Kepler\n",
    "text = \"\"\"\"(1134) Kepler – planetoida z grupy przecinających \n",
    "orbitę Marsa okrążająca Słońce w ciągu 4 lat i 145 dni \n",
    "w średniej odległości 2,68 au.\n",
    "\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italian'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://it.wikipedia.org/wiki/1134_Kepler\n",
    "text = \"\"\"1134 Kepler è un asteroide areosecante. Scoperto nel 1929, \n",
    "presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \n",
    "UA e da un'eccentricità di 0,4651458, inclinata di 15,17381° rispetto\n",
    "\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df4d8728e5b8d79bbe87bcf0244ea19b",
     "grade": true,
     "grade_id": "cell-b83cf8d4bcee2ab1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
    "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
    "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
    "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
    "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
    "         \"and eccentric Mars-crosser from the\"]\n",
    "assert mle_best(lines[0], lang_to_probs) == 'Portuguese'\n",
    "assert mle_best(lines[1], lang_to_probs) == 'Portuguese'\n",
    "assert mle_best(lines[2], lang_to_probs) == 'French'\n",
    "assert mle_best(lines[3], lang_to_probs) == 'Italian'\n",
    "assert mle_best(lines[4], lang_to_probs) == 'Polish'\n",
    "assert mle_best(lines[5], lang_to_probs) == 'English'\n",
    "assert mle_best(\"Aaaa\", lang_to_probs) == \"Portuguese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look, it works!\n",
    "Fantastic! Just a couple of equations, and we created fully automatic language detection algorithm!\n",
    "\n",
    "Now let us make it even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go Bayes\n",
    "Assume that we selected random article from all Wikipedia articles in the languages that we consider. There are different numbers of articles in different languages, so it is more likely to obtain an article e.g. in English than in Polish (at least, at this moment). It means that we need stronger evidence for Polish to accept it compared with English. To take into account this information, we will use Bayes' rule as discussed in the videos.\n",
    "\n",
    "Recall that in the Bayesian approach we consider _prior_ probabilities of languages, then use Bayes' rule to find their posterior probabilities and select the language with the largest posterior probability. We begin with finding prior probabilities. Create a dictionary `lang_to_prior` whose keys are language names and values are prior probabilities. Assume that priors are proportional to the number of articles in each language (in thousands). Let us use these numbers (in thousands): English: 6090, Italian: 1611, Spanish: 1602, German: 2439, French: 2222, Polish: 1412, Portuguese: 1034. Remember that all prior probabilities should sum up to 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "702d53d0bce478d6f65fbc96b0edb9a6",
     "grade": false,
     "grade_id": "cell-1076e90562dad99c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 0.3711151736745887, 'Italian': 0.09817184643510055, 'Spanish': 0.09762340036563072, 'German': 0.1486288848263254, 'French': 0.13540524070688603, 'Polish': 0.08604509445460086, 'Portuguese': 0.06301035953686776}\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "lang_articles = {\n",
    "    'English': 6090,\n",
    "    'Italian': 1611,\n",
    "    'Spanish': 1602,\n",
    "    'German': 2439,\n",
    "    'French': 2222,\n",
    "    'Polish': 1412,\n",
    "    'Portuguese': 1034\n",
    "}\n",
    "\n",
    "articles_ttl = sum(lang_articles.values())\n",
    "\n",
    "lang_to_prior = {key: lang_articles[key]/articles_ttl for key in lang_articles.keys()}\n",
    "\n",
    "print(lang_to_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11e4f59d59762b514409a549f44504f6",
     "grade": true,
     "grade_id": "cell-19765ecb964a7f7f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(lang_to_prior['French'] - 0.13540524070688603) < 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement function `bayesian_best(text, lang_to_probs, lang_to_prior)` that takes some text `text`, dictionary `lang_to_probs` created before and `lang_to_prior` with prior probabilities. This function has to return the language name with the largest posterior probability. Note that as we only compare posterior probabilities, we can ignore the denominator in Bayes' rule: it is the same for all languages. Note also that we need to work with logarithms of posterior instead of posteriors themself to avoid extremely small numbers. Use properties of logarithm to do it efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "216727f5af4f793b64690cff1cda4c51",
     "grade": false,
     "grade_id": "cell-1725db02a8cb77ad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key English -2.556093999491036 -0.9912428233801939 -3.54733682287123\n",
      "key French -2.6419785979002963 -1.9994832138845613 -4.641461811784858\n",
      "key German -2.8304768154846425 -1.9063027858682162 -4.7367796013528585\n",
      "key Italian -2.342416968769556 -2.321035800907162 -4.663452769676718\n",
      "key Polish -2.464604728529693 -2.4528837660309493 -4.917488494560642\n",
      "key Portuguese -2.2157575201191277 -2.764456129015762 -4.98021364913489\n",
      "key Spanish -2.30698254150174 -2.326638056455832 -4.633620597957572\n",
      "English\n",
      "key English -2.556093999491036 -0.9912428233801939 -3.54733682287123\n",
      "key French -2.6419785979002963 -1.9994832138845613 -4.641461811784858\n",
      "key German -2.8304768154846425 -1.9063027858682162 -4.7367796013528585\n",
      "key Italian -2.342416968769556 -2.321035800907162 -4.663452769676718\n",
      "key Polish -2.464604728529693 -2.4528837660309493 -4.917488494560642\n",
      "key Portuguese -2.2157575201191277 -2.764456129015762 -4.98021364913489\n",
      "key Spanish -2.30698254150174 -2.326638056455832 -4.633620597957572\n",
      "English\n",
      "key English -2.70360378366452 -0.9912428233801939 -3.694846607044714\n",
      "key French -2.612861606779676 -1.9994832138845613 -4.612344820664237\n",
      "key German -2.7268051399864186 -1.9063027858682162 -4.633107925854635\n",
      "key Italian -2.6561837813554705 -2.321035800907162 -4.977219582262633\n",
      "key Polish -2.8367414516305676 -2.4528837660309493 -5.289625217661516\n",
      "key Portuguese -2.6595202693939415 -2.764456129015762 -5.423976398409703\n",
      "key Spanish -2.629042889716838 -2.326638056455832 -4.95568094617267\n",
      "English\n",
      "key English -2.8057163758234274 -0.9912428233801939 -3.7969591992036213\n",
      "key French -2.7871715176913154 -1.9994832138845613 -4.786654731575877\n",
      "key German -2.7910145037239342 -1.9063027858682162 -4.69731728959215\n",
      "key Italian -2.721114606904673 -2.321035800907162 -5.042150407811835\n",
      "key Polish -2.849315506975484 -2.4528837660309493 -5.302199273006433\n",
      "key Portuguese -2.758632215965771 -2.764456129015762 -5.523088344981533\n",
      "key Spanish -2.75944018889857 -2.326638056455832 -5.086078245354402\n",
      "English\n",
      "key English -3.0581840321571065 -0.9912428233801939 -4.0494268555373\n",
      "key French -3.115872381775211 -1.9994832138845613 -5.115355595659772\n",
      "key German -3.0907602334820385 -1.9063027858682162 -4.9970630193502545\n",
      "key Italian -3.041845096415251 -2.321035800907162 -5.3628808973224125\n",
      "key Polish -2.9037071314145195 -2.4528837660309493 -5.356590897445469\n",
      "key Portuguese -3.0728303815115683 -2.764456129015762 -5.83728651052733\n",
      "key Spanish -3.031763555458751 -2.326638056455832 -5.358401611914584\n",
      "English\n",
      "key English -2.7805827324334893 -0.9912428233801939 -3.7718255558136833\n",
      "key French -2.8072912489141246 -1.9994832138845613 -4.8067744627986855\n",
      "key German -2.8454543333062965 -1.9063027858682162 -4.7517571191745125\n",
      "key Italian -2.7945156916506937 -2.321035800907162 -5.115551492557856\n",
      "key Polish -2.926301629623805 -2.4528837660309493 -5.379185395654755\n",
      "key Portuguese -2.7968249642212877 -2.764456129015762 -5.561281093237049\n",
      "key Spanish -2.78348513611484 -2.326638056455832 -5.110123192570672\n",
      "English\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def bayesian_best(text, lang_to_probs, lang_to_prior):\n",
    "    freqs = get_freqs(clean_text(text), True)\n",
    "    likelihood = mle_best(text, lang_to_probs, True)\n",
    "    res = {}\n",
    "\n",
    "    for key in likelihood.keys():\n",
    "        print('key', key, likelihood[key], math.log(lang_to_prior[key]), likelihood[key] + math.log(lang_to_prior[key]))\n",
    "        res[key] = likelihood[key] + math.log(lang_to_prior[key])\n",
    "    \n",
    "\n",
    "    return max(res, key=lambda key: res[key])\n",
    "\n",
    "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
    "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
    "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
    "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
    "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
    "         \"and eccentric Mars-crosser from the\"]\n",
    "answers = ['English', 'English', 'French', 'Italian', 'Polish', 'English']\n",
    "\n",
    "for line, answer in zip(lines, answers):\n",
    "    print(bayesian_best(line, lang_to_probs, lang_to_prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, MLE algorithm believes that word `\"The\"` belongs go German language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'German'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_best(\"The\", lang_to_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we take into account that English is more popular in Wikipedia, the results changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key English -2.8434741872582263 -0.9912428233801939 -3.8347170106384203\n",
      "key French -3.3039505374577938 -1.9994832138845613 -5.3034337513423555\n",
      "key German -2.730045843356664 -1.9063027858682162 -4.63634862922488\n",
      "key Italian -3.5816633147693637 -2.321035800907162 -5.902699115676526\n",
      "key Polish -3.52414439992688 -2.4528837660309493 -5.9770281659578295\n",
      "key Portuguese -3.5079318432481092 -2.764456129015762 -6.272387972263871\n",
      "key Spanish -3.6002692362659103 -2.326638056455832 -5.926907292721742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_best(\"The\", lang_to_probs, lang_to_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3df29258fc53bca55abeea29456124f",
     "grade": true,
     "grade_id": "cell-d6b6b54026f24fb0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key English -2.556093999491036 -0.9912428233801939 -3.54733682287123\n",
      "key French -2.6419785979002963 -1.9994832138845613 -4.641461811784858\n",
      "key German -2.8304768154846425 -1.9063027858682162 -4.7367796013528585\n",
      "key Italian -2.342416968769556 -2.321035800907162 -4.663452769676718\n",
      "key Polish -2.464604728529693 -2.4528837660309493 -4.917488494560642\n",
      "key Portuguese -2.2157575201191277 -2.764456129015762 -4.98021364913489\n",
      "key Spanish -2.30698254150174 -2.326638056455832 -4.633620597957572\n",
      "key English -2.556093999491036 -0.9912428233801939 -3.54733682287123\n",
      "key French -2.6419785979002963 -1.9994832138845613 -4.641461811784858\n",
      "key German -2.8304768154846425 -1.9063027858682162 -4.7367796013528585\n",
      "key Italian -2.342416968769556 -2.321035800907162 -4.663452769676718\n",
      "key Polish -2.464604728529693 -2.4528837660309493 -4.917488494560642\n",
      "key Portuguese -2.2157575201191277 -2.764456129015762 -4.98021364913489\n",
      "key Spanish -2.30698254150174 -2.326638056455832 -4.633620597957572\n",
      "key English -2.70360378366452 -0.9912428233801939 -3.694846607044714\n",
      "key French -2.612861606779676 -1.9994832138845613 -4.612344820664237\n",
      "key German -2.7268051399864186 -1.9063027858682162 -4.633107925854635\n",
      "key Italian -2.6561837813554705 -2.321035800907162 -4.977219582262633\n",
      "key Polish -2.8367414516305676 -2.4528837660309493 -5.289625217661516\n",
      "key Portuguese -2.6595202693939415 -2.764456129015762 -5.423976398409703\n",
      "key Spanish -2.629042889716838 -2.326638056455832 -4.95568094617267\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-5e08c2a41c43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'English'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'English'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'French'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Italian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Polish'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'English'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mbayesian_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_to_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_to_prior\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
    "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
    "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
    "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
    "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
    "         \"and eccentric Mars-crosser from the\"]\n",
    "answers = ['English', 'English', 'French', 'Italian', 'Polish', 'English']\n",
    "for line, answer in zip(lines, answers):\n",
    "    assert bayesian_best(line, lang_to_probs, lang_to_prior) == answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring uncertainty\n",
    "Finally, let us get posterior probability of how certain our Bayesian algorithm is in its classification. Implement function `bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang)` that takes some `text`, `lang_to_probs`, `lang_to_prior` and a particular language `test_lang` we are interested in and returns posterior probability for this language provided this text. Use formula for the posterior from the videos.\n",
    "\n",
    "Note that in this case you have to work with actual probabilities (likelihoods), not their logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf52836c632b7285e461a95c4e15cac1",
     "grade": false,
     "grade_id": "cell-8705f891f8eff4f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cec1793d2b6f5143a7bbaf9e1c3f0bd",
     "grade": true,
     "grade_id": "cell-23ecc58928d8d618",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\") - 0.26863814640) < 0.00001\n",
    "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\") - 0.534626604) < 0.00001\n",
    "assert abs(bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\") - 0.365967931517) < 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our algorithm believes that `\"Das\"` belongs to English. This is due to the small amount of data (only three letters!) and high prior for English. However, it is not very certain: the posterior is only 0.37. On the other hand, `\"The\"` belongs to `\"English\"` with much larger posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Today we used our knowledge of probability to construct our own classifier algorithm. Actually, it is a version of a well-known naive Bayesian classifier. Of course, this classifier is far from perfect: for example, it completely ignores words and deals only with characters and their frequencies. Nevertheless, it works rather well despite being so simple. Also it shows several important concepts: likelihood, maximum likelihood estimation, Bayesian estimations and so on.\n",
    "\n",
    "Now you are ready for more sophisticated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P.S. Efficiency considerations\n",
    "During this project we used dictionaries to represent frequency tables and pure Python constructions like loops to process them. However, it is much more efficient to use `numpy` arrays and vectorized functions. You can try to redo this project with `numpy` if you know how to use it. If you don't know yet, that's not a problem: you will learn it soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
